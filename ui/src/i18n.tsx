import { createContext, ReactNode, useContext, useEffect, useMemo, useState } from "react";

type Locale = "en" | "es" | "fr" | "de" | "ar" | "tr" | "fa" | "zh" | "hi";

const translations: Record<Locale, Record<string, string>> = {
  en: {
    brand: "AURORA • FinAI Labz",
    navChat: "Chat",
    navModels: "Models",
    navPull: "Pull/Search",
    navSettings: "Settings",
    navLogs: "Logs",
    modelsHubTitle: "Models · install & search",
    manualInstall: "Manual install (GGUF)",
    remove: "Remove",
    removing: "Removing...",
    removed: "Removed",
    chatTitle: "Chat",
    completionTitle: "One-shot Completion",
    modelLabel: "Model",
    promptLabel: "Prompt",
    promptPlaceholder: "Ask something...",
    send: "Send",
    thinking: "Thinking...",
    generate: "Generate",
    generating: "Generating...",
    output: "Output",
    installedModels: "Installed Models",
    noModels: "No models yet. Pull one to start.",
    source: "source",
    pullTitle: "Pull / Search Models",
    localName: "Local name",
    repo: "Repo",
    filename: "Filename",
    subfolder: "Subfolder (optional)",
    queue: "Queue Download",
    queued: "Queued",
    error: "Error",
    searchLabel: "Search FinAI catalog (GGUF)",
    searchPlaceholder: "glm, llama-3, qwen...",
    noResults: "No results",
    use: "Use",
    downloadsLikes: "downloads · likes",
    searching: "Searching...",
    settingsTitle: "Settings",
    storageDir: "Storage Dir",
    llamaPath: "llama-server Path",
    llamaArgs: "llama-server Args",
    llamaHost: "llama-server Host",
    llamaPort: "llama-server Port",
    defaultModel: "Default Model",
    save: "Save Settings",
    saving: "Saving...",
    saved: "Saved.",
    logsTitle: "Logs",
    liveFrom: "Live from",
    language: "Language",
    favoritesTitle: "Saved Prompts",
    savePrompt: "Save prompt",
    promptName: "Prompt name",
    improvePrompt: "Improve prompt",
    improved: "Improved prompt",
    navHelp: "Help",
    helpTitle: "How to use Aurora",
    helpShortcuts: "Tips",
    helpSend: "Use Chat for multi-turn or Completion for one-shot. Set a model and hit Send.",
    helpPull: "Pull or search models from the Models tab; use GGUF filenames.",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "Subscriptions for heavy models",
    planBasic: "Starter: local only (no GPU billing).",
    planPro: "Pro GPU: for GLM-4.7+/MiniMax M2 and heavier models.",
    planEnterprise: "Enterprise: dedicated GPUs and custom limits.",
    planCta: "Manage or subscribe",
    navTerminal: "CLI",
    terminalTitle: "Embedded CLI",
    terminalHelp: "Commands: help, list, pull <name> <repo> <filename> [subfolder], run <name>.",
    terminalPlaceholder: "Type a command, e.g., pull glm org/repo model.gguf",
  },
  es: {
    brand: "AURORA • FinAI Labz",
    navChat: "Chat",
    navModels: "Modelos",
    navPull: "Descargar/Buscar",
    navSettings: "Ajustes",
    navLogs: "Logs",
    modelsHubTitle: "Modelos · instalar y buscar",
    manualInstall: "Instalación manual (GGUF)",
    remove: "Eliminar",
    removing: "Eliminando...",
    removed: "Eliminado",
    chatTitle: "Chat",
    completionTitle: "Compleción puntual",
    modelLabel: "Modelo",
    promptLabel: "Prompt",
    promptPlaceholder: "Pregunta algo...",
    send: "Enviar",
    thinking: "Pensando...",
    generate: "Generar",
    generating: "Generando...",
    output: "Salida",
    installedModels: "Modelos instalados",
    noModels: "Sin modelos aún. Descarga uno para empezar.",
    source: "origen",
    pullTitle: "Descargar / Buscar modelos",
    localName: "Nombre local",
    repo: "Repositorio",
    filename: "Archivo",
    subfolder: "Subcarpeta (opcional)",
    queue: "Encolar descarga",
    queued: "Encolado",
    error: "Error",
    searchLabel: "Buscar en catálogo FinAI (GGUF)",
    searchPlaceholder: "glm, llama-3, qwen...",
    noResults: "Sin resultados",
    use: "Usar",
    downloadsLikes: "descargas · me gusta",
    searching: "Buscando...",
    settingsTitle: "Ajustes",
    storageDir: "Carpeta de modelos",
    llamaPath: "Ruta llama-server",
    llamaArgs: "Args llama-server",
    llamaHost: "Host llama-server",
    llamaPort: "Puerto llama-server",
    defaultModel: "Modelo por defecto",
    save: "Guardar ajustes",
    saving: "Guardando...",
    saved: "Guardado.",
    logsTitle: "Logs",
    liveFrom: "En vivo desde",
    language: "Idioma",
    favoritesTitle: "Prompts guardados",
    savePrompt: "Guardar prompt",
    promptName: "Nombre del prompt",
    improvePrompt: "Mejorar prompt",
    improved: "Prompt mejorado",
    navHelp: "Ayuda",
    helpTitle: "Cómo usar Aurora",
    helpShortcuts: "Consejos",
    helpSend: "Usa Chat para varias vueltas o Completion para una sola. Elige modelo y envía.",
    helpPull: "Descarga o busca modelos en la pestaña Modelos; usa archivos GGUF.",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "Suscripciones para modelos pesados",
    planBasic: "Inicio: solo local (sin facturación GPU).",
    planPro: "Pro GPU: para GLM-4.7+/MiniMax M2 y modelos pesados.",
    planEnterprise: "Enterprise: GPU dedicadas y límites personalizados.",
    planCta: "Gestionar o suscribirse",
  },
  fr: {
    brand: "AURORA • FinAI Labz",
    navChat: "Chat",
    navModels: "Modèles",
    navPull: "Télécharger/Chercher",
    navSettings: "Réglages",
    navLogs: "Logs",
    modelsHubTitle: "Modèles · installer et chercher",
    manualInstall: "Installation manuelle (GGUF)",
    remove: "Supprimer",
    removing: "Suppression...",
    removed: "Supprimé",
    chatTitle: "Chat",
    completionTitle: "Complétion directe",
    modelLabel: "Modèle",
    promptLabel: "Invite",
    promptPlaceholder: "Posez une question...",
    send: "Envoyer",
    thinking: "Réflexion...",
    generate: "Générer",
    generating: "Génération...",
    output: "Résultat",
    installedModels: "Modèles installés",
    noModels: "Aucun modèle. Téléchargez-en un pour commencer.",
    source: "source",
    pullTitle: "Télécharger / Chercher des modèles",
    localName: "Nom local",
    repo: "Dépôt",
    filename: "Fichier",
    subfolder: "Sous-dossier (optionnel)",
    queue: "Mettre en file",
    queued: "En file",
    error: "Erreur",
    searchLabel: "Recherche catalogue FinAI (GGUF)",
    searchPlaceholder: "glm, llama-3, qwen...",
    noResults: "Aucun résultat",
    use: "Utiliser",
    downloadsLikes: "téléchargements · likes",
    searching: "Recherche...",
    settingsTitle: "Réglages",
    storageDir: "Dossier des modèles",
    llamaPath: "Chemin llama-server",
    llamaArgs: "Arguments llama-server",
    llamaHost: "Hôte llama-server",
    llamaPort: "Port llama-server",
    defaultModel: "Modèle par défaut",
    save: "Enregistrer",
    saving: "Enregistrement...",
    saved: "Enregistré.",
    logsTitle: "Logs",
    liveFrom: "En direct depuis",
    language: "Langue",
    favoritesTitle: "Prompts enregistrés",
    savePrompt: "Enregistrer le prompt",
    promptName: "Nom du prompt",
    improvePrompt: "Améliorer le prompt",
    improved: "Prompt amélioré",
    navHelp: "Aide",
    helpTitle: "Comment utiliser Aurora",
    helpShortcuts: "Astuces",
    helpSend: "Chat pour plusieurs tours, Completion pour un seul. Choisissez un modèle et envoyez.",
    helpPull: "Téléchargez ou cherchez des modèles dans l’onglet Modèles ; utilisez des fichiers GGUF.",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "Abonnements pour modèles lourds",
    planBasic: "Starter : local uniquement (pas de facturation GPU).",
    planPro: "Pro GPU : pour GLM-4.7+/MiniMax M2 et modèles lourds.",
    planEnterprise: "Entreprise : GPU dédiés et limites sur mesure.",
    planCta: "Gérer ou souscrire",
  },
  de: {
    brand: "AURORA • FinAI Labz",
    navChat: "Chat",
    navModels: "Modelle",
    navPull: "Laden/Suchen",
    navSettings: "Einstellungen",
    navLogs: "Logs",
    modelsHubTitle: "Modelle · installieren & suchen",
    manualInstall: "Manuelle Installation (GGUF)",
    remove: "Entfernen",
    removing: "Wird entfernt...",
    removed: "Entfernt",
    chatTitle: "Chat",
    completionTitle: "Einmalige Completion",
    modelLabel: "Modell",
    promptLabel: "Prompt",
    promptPlaceholder: "Frag etwas...",
    send: "Senden",
    thinking: "Denkt...",
    generate: "Generieren",
    generating: "Generiert...",
    output: "Ausgabe",
    installedModels: "Installierte Modelle",
    noModels: "Noch keine Modelle. Lade eines herunter.",
    source: "Quelle",
    pullTitle: "Modelle laden / suchen",
    localName: "Lokaler Name",
    repo: "Repo",
    filename: "Datei",
    subfolder: "Unterordner (optional)",
    queue: "Download einreihen",
    queued: "Eingereiht",
    error: "Fehler",
    searchLabel: "FinAI-Katalog durchsuchen (GGUF)",
    searchPlaceholder: "glm, llama-3, qwen...",
    noResults: "Keine Treffer",
    use: "Übernehmen",
    downloadsLikes: "Downloads · Likes",
    searching: "Suche...",
    settingsTitle: "Einstellungen",
    storageDir: "Modellordner",
    llamaPath: "Pfad zu llama-server",
    llamaArgs: "Argumente llama-server",
    llamaHost: "Host llama-server",
    llamaPort: "Port llama-server",
    defaultModel: "Standardmodell",
    save: "Einstellungen speichern",
    saving: "Speichern...",
    saved: "Gespeichert.",
    logsTitle: "Logs",
    liveFrom: "Live von",
    language: "Sprache",
    favoritesTitle: "Gespeicherte Prompts",
    savePrompt: "Prompt speichern",
    promptName: "Prompt-Name",
    improvePrompt: "Prompt verbessern",
    improved: "Verbesserter Prompt",
    navHelp: "Hilfe",
    helpTitle: "Aurora verwenden",
    helpShortcuts: "Tipps",
    helpSend: "Chat für mehrere Runden, Completion für Einmal-Antwort. Modell wählen und senden.",
    helpPull: "Modelle im Tab Modelle laden oder suchen; GGUF-Dateien nutzen.",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "Abos für schwere Modelle",
    planBasic: "Starter: nur lokal (keine GPU-Kosten).",
    planPro: "Pro GPU: für GLM-4.7+/MiniMax M2 und schwere Modelle.",
    planEnterprise: "Enterprise: dedizierte GPUs, individuelle Limits.",
    planCta: "Verwalten oder abonnieren",
  },
  ar: {
    brand: "AURORA • FinAI Labz",
    navChat: "دردشة",
    navModels: "النماذج",
    navPull: "تنزيل/بحث",
    navSettings: "الإعدادات",
    navLogs: "السجلات",
    modelsHubTitle: "النماذج · تثبيت وبحث",
    manualInstall: "تثبيت يدوي (GGUF)",
    remove: "حذف",
    removing: "جارٍ الحذف...",
    removed: "تم الحذف",
    chatTitle: "الدردشة",
    completionTitle: "إكمال فوري",
    modelLabel: "النموذج",
    promptLabel: "الموجه",
    promptPlaceholder: "اكتب سؤالاً...",
    send: "إرسال",
    thinking: "جارٍ التفكير...",
    generate: "توليد",
    generating: "جارٍ التوليد...",
    output: "النتيجة",
    installedModels: "النماذج المثبتة",
    noModels: "لا توجد نماذج بعد. قم بتنزيل واحد للبدء.",
    source: "المصدر",
    pullTitle: "تنزيل / البحث عن النماذج",
    localName: "الاسم المحلي",
    repo: "المستودع",
    filename: "اسم الملف",
    subfolder: "مجلد فرعي (اختياري)",
    queue: "إضافة للتنزيل",
    queued: "تمت الإضافة",
    error: "خطأ",
    searchLabel: "ابحث في كتالوج FinAI (GGUF)",
    searchPlaceholder: "glm أو llama-3 أو qwen...",
    noResults: "لا توجد نتائج",
    use: "استخدام",
    downloadsLikes: "تنزيلات · إعجابات",
    searching: "جارٍ البحث...",
    settingsTitle: "الإعدادات",
    storageDir: "مجلد النماذج",
    llamaPath: "مسار llama-server",
    llamaArgs: "وسائط llama-server",
    llamaHost: "مضيف llama-server",
    llamaPort: "منفذ llama-server",
    defaultModel: "النموذج الافتراضي",
    save: "حفظ الإعدادات",
    saving: "جارٍ الحفظ...",
    saved: "تم الحفظ.",
    logsTitle: "السجلات",
    liveFrom: "مباشر من",
    language: "اللغة",
    favoritesTitle: "المطالبات المحفوظة",
    savePrompt: "حفظ الموجه",
    promptName: "اسم الموجه",
    improvePrompt: "تحسين الموجه",
    improved: "الموجه المحسّن",
    navHelp: "مساعدة",
    helpTitle: "كيفية استخدام Aurora",
    helpShortcuts: "نصائح",
    helpSend: "استخدم الدردشة لعدة رسائل أو الإكمال لرسالة واحدة. اختر النموذج ثم أرسل.",
    helpPull: "نزّل أو ابحث عن النماذج من تبويب النماذج؛ استخدم ملفات GGUF.",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "الاشتراكات للنماذج الثقيلة",
    planBasic: "مبتدئ: محلي فقط (بدون تكلفة GPU).",
    planPro: "Pro GPU: لـ GLM-4.7+/MiniMax M2 والنماذج الثقيلة.",
    planEnterprise: "Enterprise: GPU مخصصة وحدود مخصصة.",
    planCta: "إدارة أو اشتراك",
  },
  tr: {
    brand: "AURORA • FinAI Labz",
    navChat: "Sohbet",
    navModels: "Modeller",
    navPull: "İndir/Ara",
    navSettings: "Ayarlar",
    navLogs: "Kayıtlar",
    modelsHubTitle: "Modeller · kur/ara",
    manualInstall: "Manuel kurulum (GGUF)",
    remove: "Sil",
    removing: "Siliniyor...",
    removed: "Silindi",
    chatTitle: "Sohbet",
    completionTitle: "Tek seferlik üretim",
    modelLabel: "Model",
    promptLabel: "İstek",
    promptPlaceholder: "Bir şey sor...",
    send: "Gönder",
    thinking: "Düşünüyor...",
    generate: "Üret",
    generating: "Üretiliyor...",
    output: "Çıktı",
    installedModels: "Yüklü Modeller",
    noModels: "Henüz model yok. Başlamak için birini indir.",
    source: "kaynak",
    pullTitle: "Model indir / ara",
    localName: "Yerel ad",
    repo: "Depo",
    filename: "Dosya adı",
    subfolder: "Alt klasör (opsiyonel)",
    queue: "İndirmeye ekle",
    queued: "Eklendi",
    error: "Hata",
    searchLabel: "FinAI kataloğunda ara (GGUF)",
    searchPlaceholder: "glm, llama-3, qwen...",
    noResults: "Sonuç yok",
    use: "Kullan",
    downloadsLikes: "indirme · beğeni",
    searching: "Aranıyor...",
    settingsTitle: "Ayarlar",
    storageDir: "Model klasörü",
    llamaPath: "llama-server yolu",
    llamaArgs: "llama-server argümanları",
    llamaHost: "llama-server host",
    llamaPort: "llama-server port",
    defaultModel: "Varsayılan model",
    save: "Ayarları kaydet",
    saving: "Kaydediliyor...",
    saved: "Kaydedildi.",
    logsTitle: "Kayıtlar",
    liveFrom: "Canlı",
    language: "Dil",
    favoritesTitle: "Kayıtlı Prompts",
    savePrompt: "Promptu kaydet",
    promptName: "Prompt adı",
    improvePrompt: "Promptu iyileştir",
    improved: "İyileştirilmiş prompt",
    navHelp: "Yardım",
    helpTitle: "Aurora nasıl kullanılır",
    helpShortcuts: "İpuçları",
    helpSend: "Chat çok turlu, Completion tek seferlik için. Model seç ve gönder.",
    helpPull: "Modeller sekmesinde modelleri indir ya da ara; GGUF dosyaları kullan.",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "Ağır modeller için abonelikler",
    planBasic: "Başlangıç: yalnızca lokal (GPU ücreti yok).",
    planPro: "Pro GPU: GLM-4.7+/MiniMax M2 ve ağır modeller için.",
    planEnterprise: "Kurumsal: özel GPU ve özelleştirilmiş limitler.",
    planCta: "Yönet veya abone ol",
  },
  fa: {
    brand: "AURORA • FinAI Labz",
    navChat: "گفتگو",
    navModels: "مدل‌ها",
    navPull: "دانلود/جستجو",
    navSettings: "تنظیمات",
    navLogs: "لاگ‌ها",
    modelsHubTitle: "مدل‌ها · نصب و جستجو",
    manualInstall: "نصب دستی (GGUF)",
    remove: "حذف",
    removing: "در حال حذف...",
    removed: "حذف شد",
    chatTitle: "گفتگو",
    completionTitle: "تولید تک‌مرحله‌ای",
    modelLabel: "مدل",
    promptLabel: "پرامپت",
    promptPlaceholder: "سوالی بپرسید...",
    send: "ارسال",
    thinking: "در حال فکر...",
    generate: "تولید",
    generating: "در حال تولید...",
    output: "خروجی",
    installedModels: "مدل‌های نصب‌شده",
    noModels: "هنوز مدلی نیست. یکی دانلود کنید.",
    source: "منبع",
    pullTitle: "دانلود / جستجوی مدل",
    localName: "نام محلی",
    repo: "مخزن",
    filename: "نام فایل",
    subfolder: "زیرپوشه (اختیاری)",
    queue: "افزودن به دانلود",
    queued: "افزوده شد",
    error: "خطا",
    searchLabel: "جستجو در کاتالوگ FinAI (GGUF)",
    searchPlaceholder: "glm یا llama-3 یا qwen...",
    noResults: "یافت نشد",
    use: "استفاده",
    downloadsLikes: "دانلود · لایک",
    searching: "در حال جستجو...",
    settingsTitle: "تنظیمات",
    storageDir: "پوشه مدل‌ها",
    llamaPath: "مسیر llama-server",
    llamaArgs: "آرگومان‌های llama-server",
    llamaHost: "هاست llama-server",
    llamaPort: "پورت llama-server",
    defaultModel: "مدل پیش‌فرض",
    save: "ذخیره تنظیمات",
    saving: "در حال ذخیره...",
    saved: "ذخیره شد.",
    logsTitle: "لاگ‌ها",
    liveFrom: "زنده از",
    language: "زبان",
    favoritesTitle: "پرامپت‌های ذخیره‌شده",
    savePrompt: "ذخیره پرامپت",
    promptName: "نام پرامپت",
    improvePrompt: "بهبود پرامپت",
    improved: "پرامپت بهبود یافته",
    navHelp: "راهنما",
    helpTitle: "چطور از Aurora استفاده کنم",
    helpShortcuts: "نکات",
    helpSend: "چت برای چند نوبت و Completion برای یک پاسخ. مدل را انتخاب کنید و ارسال کنید.",
    helpPull: "مدل‌ها را در تب مدل‌ها دانلود یا جستجو کنید؛ از فایل‌های GGUF استفاده کنید.",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "اشتراک برای مدل‌های سنگین",
    planBasic: "شروع: فقط محلی (بدون هزینه GPU).",
    planPro: "Pro GPU: برای GLM-4.7+/MiniMax M2 و مدل‌های سنگین.",
    planEnterprise: "Enterprise: GPU اختصاصی و محدودیت‌های سفارشی.",
    planCta: "مدیریت یا اشتراک",
  },
  zh: {
    brand: "AURORA • FinAI Labz",
    navChat: "对话",
    navModels: "模型",
    navPull: "下载/搜索",
    navSettings: "设置",
    navLogs: "日志",
    modelsHubTitle: "模型 · 安装/搜索",
    manualInstall: "手动安装 (GGUF)",
    remove: "删除",
    removing: "删除中…",
    removed: "已删除",
    chatTitle: "对话",
    completionTitle: "单次补全",
    modelLabel: "模型",
    promptLabel: "提示",
    promptPlaceholder: "问点什么…",
    send: "发送",
    thinking: "思考中…",
    generate: "生成",
    generating: "生成中…",
    output: "输出",
    installedModels: "已安装模型",
    noModels: "尚无模型。先下载一个。",
    source: "来源",
    pullTitle: "下载 / 搜索模型",
    localName: "本地名称",
    repo: "仓库",
    filename: "文件名",
    subfolder: "子目录（可选）",
    queue: "加入下载队列",
    queued: "已加入",
    error: "错误",
    searchLabel: "搜索 FinAI 目录（GGUF）",
    searchPlaceholder: "glm、llama-3、qwen…",
    noResults: "无结果",
    use: "使用",
    downloadsLikes: "下载 · 喜欢",
    searching: "搜索中…",
    settingsTitle: "设置",
    storageDir: "模型目录",
    llamaPath: "llama-server 路径",
    llamaArgs: "llama-server 参数",
    llamaHost: "llama-server 主机",
    llamaPort: "llama-server 端口",
    defaultModel: "默认模型",
    save: "保存设置",
    saving: "保存中…",
    saved: "已保存。",
    logsTitle: "日志",
    liveFrom: "实时来自",
    language: "语言",
    favoritesTitle: "已保存提示",
    savePrompt: "保存提示",
    promptName: "提示名称",
    improvePrompt: "优化提示",
    improved: "优化后的提示",
    navHelp: "帮助",
    helpTitle: "如何使用 Aurora",
    helpShortcuts: "提示",
    helpSend: "对话用于多轮，补全用于单轮。先选模型，再发送。",
    helpPull: "在“模型”标签页下载或搜索模型，使用 GGUF 文件名。",
    helpSettings: "Choose your storage folder and default model; the Rust backend loads GGUFs from there.",
    helpLogs: "Logs show the Rust backend and model inference activity.",
    helpCli: "Use the embedded CLI panel (help/list/pull/run/chat) to manage models.",
    plansTitle: "重型模型订阅",
    planBasic: "入门：仅本地（无 GPU 费用）。",
    planPro: "Pro GPU：适用于 GLM-4.7+/MiniMax M2 等大模型。",
    planEnterprise: "企业版：专用 GPU 与自定义配额。",
    planCta: "管理或订阅",
  },
  hi: {
    brand: "AURORA • FinAI Labz",
    navChat: "चैट",
    navModels: "मॉडल",
    navPull: "डाउनलोड/खोज",
    navSettings: "सेटिंग्स",
    navLogs: "लॉग",
    modelsHubTitle: "मॉडल · इंस्टॉल और खोजें",
    manualInstall: "मैन्युअल इंस्टॉल (GGUF)",
    remove: "हटाएं",
    removing: "हटा रहे हैं...",
    removed: "हटा दिया गया",
    chatTitle: "चैट",
    completionTitle: "एक-शॉट कंप्लीशन",
    modelLabel: "मॉडल",
    promptLabel: "प्रॉम्प्ट",
    promptPlaceholder: "कुछ पूछें...",
    send: "भेजें",
    thinking: "सोच रहा है...",
    generate: "जनरेट करें",
    generating: "जनरेट हो रहा है...",
    output: "आउटपुट",
    installedModels: "इंस्टॉल किए गए मॉडल",
    noModels: "अभी कोई मॉडल नहीं। शुरू करने के लिए एक डाउनलोड करें।",
    source: "स्रोत",
    pullTitle: "मॉडल डाउनलोड / खोजें",
    localName: "लोकल नाम",
    repo: "रेपो",
    filename: "फाइल नाम",
    subfolder: "सबफोल्डर (वैकल्पिक)",
    queue: "डाउनलोड में जोड़ें",
    queued: "जोड़ दिया गया",
    error: "त्रुटि",
    searchLabel: "FinAI कैटलॉग में खोजें (GGUF)",
    searchPlaceholder: "glm, llama-3, qwen...",
    noResults: "कोई परिणाम नहीं",
    use: "उपयोग करें",
    downloadsLikes: "डाउनलोड · पसंद",
    searching: "खोज रहे हैं...",
    settingsTitle: "सेटिंग्स",
    storageDir: "मॉडल फोल्डर",
    llamaPath: "llama-server पथ",
    llamaArgs: "llama-server आर्ग्युमेंट्स",
    llamaHost: "llama-server होस्ट",
    llamaPort: "llama-server पोर्ट",
    defaultModel: "डिफ़ॉल्ट मॉडल",
    save: "सेटिंग्स सेव करें",
    saving: "सेव हो रहा है...",
    saved: "सेव हो गया।",
    logsTitle: "लॉग",
    liveFrom: "लाइव से",
    language: "भाषा",
    favoritesTitle: "सेव किए गए प्रॉम्प्ट्स",
    savePrompt: "प्रॉम्प्ट सेव करें",
    promptName: "प्रॉम्प्ट का नाम",
    improvePrompt: "प्रॉम्प्ट सुधारें",
    improved: "सुधारा हुआ प्रॉम्प्ट",
    navHelp: "सहायता",
    helpTitle: "Aurora का उपयोग कैसे करें",
    helpShortcuts: "टिप्स",
    helpSend: "मल्टी-टर्न के लिए चैट या सिंगल-शॉट के लिए कंप्लीशन का उपयोग करें। मॉडल चुनें और भेजें।",
    helpPull: "मॉडल टैब से मॉडल डाउनलोड या खोजें; GGUF फाइल नाम का उपयोग करें।",
    helpSettings: "अपना स्टोरेज फोल्डर और डिफ़ॉल्ट मॉडल चुनें; Rust बैकएंड वहां से GGUFs लोड करता है।",
    helpLogs: "लॉग Rust बैकएंड और मॉडल इंफरेंस गतिविधि दिखाते हैं।",
    helpCli: "मॉडल प्रबंधित करने के लिए एम्बेडेड CLI पैनल (help/list/pull/run/chat) का उपयोग करें।",
    plansTitle: "भारी मॉडल के लिए सब्सक्रिप्शन",
    planBasic: "स्टार्टर: केवल लोकल (कोई GPU बिलिंग नहीं)।",
    planPro: "Pro GPU: GLM-4.7+/MiniMax M2 और भारी मॉडल के लिए।",
    planEnterprise: "Enterprise: समर्पित GPU और कस्टम लिमिट।",
    planCta: "प्रबंधित करें या सब्सक्राइब करें",
    navTerminal: "CLI",
    terminalTitle: "एम्बेडेड CLI",
    terminalHelp: "कमांड्स: help, list, pull <name> <repo> <filename> [subfolder], run <name>।",
    terminalPlaceholder: "कमांड टाइप करें, जैसे pull glm org/repo model.gguf",
    noSaved: "कोई सेव किया हुआ प्रॉम्प्ट नहीं",
    connecting: "कनेक्ट हो रहा है...",
    online: "ऑनलाइन",
    offline: "ऑफलाइन",
    none: "कोई नहीं",
    tokens: "टोकन",
  },
};

type I18nContextShape = {
  locale: Locale;
  setLocale: (locale: Locale) => void;
  t: (key: keyof typeof translations["en"]) => string;
};

const I18nContext = createContext<I18nContextShape>({
  locale: "en",
  setLocale: () => {},
  t: (key) => translations.en[key],
});

const STORAGE_KEY = "aurora_locale";

function detectLocale(): Locale {
  const stored = (typeof localStorage !== "undefined" && (localStorage.getItem(STORAGE_KEY) as Locale | null)) || null;
  if (stored && translations[stored]) return stored;
  if (typeof navigator !== "undefined") {
    const candidates = [
      ...(navigator.languages || []),
      navigator.language,
      (navigator as any).userLanguage,
      (navigator as any).browserLanguage,
    ].filter(Boolean) as string[];
    for (const cand of candidates) {
      const norm = cand.toLowerCase();
      if (norm.startsWith("en")) return "en";
      if (norm.startsWith("es")) return "es";
      if (norm.startsWith("fr")) return "fr";
      if (norm.startsWith("de")) return "de";
      if (norm.startsWith("ar")) return "ar";
      if (norm.startsWith("tr")) return "tr";
      if (norm.startsWith("fa") || norm.startsWith("pr") || norm.startsWith("ir")) return "fa";
      if (norm.startsWith("zh") || norm.startsWith("cn")) return "zh";
      if (norm.startsWith("hi") || norm.startsWith("in")) return "hi";
    }
  }
  return "en";
}

export function I18nProvider({ children }: { children: ReactNode }) {
  const [locale, setLocaleState] = useState<Locale>(() => detectLocale());

  useEffect(() => {
    if (typeof localStorage !== "undefined") {
      localStorage.setItem(STORAGE_KEY, locale);
    }
  }, [locale]);

  const t = useMemo(() => {
    return (key: keyof typeof translations["en"]) => translations[locale]?.[key] ?? translations.en[key];
  }, [locale]);

  const setLocale = (next: Locale) => setLocaleState(next);

  return <I18nContext.Provider value={{ locale, setLocale, t }}>{children}</I18nContext.Provider>;
}

export function useI18n() {
  return useContext(I18nContext);
}
